---
layout: post
title: "2.8 Information theory"
category: mlapp
---

###Information theory

信息论与如何紧凑的数据表示有紧密关系，如数据压缩、源编码，同样与如何健壮的传输和存储也有着密切的关系。
看上去它与概率论、机器学习没有什么直接的关系，其实他们关系很密切的。
例如：在紧凑表示数据的时候，需要分配交短的编码给出现频率较高的字符，分配较长的编码给出现频率较低的字符。这跟自然语言处理中的处理方式很类似，a, an, the 这类词的编码要远小于不常见词的编码。
在Cover and Thomas 2006的书上有关于信息论的详细介绍，本节主要介绍熵、KL距离和互信息。

###熵

熵，用来表示混乱程度。当一个变量的混乱程度越大，则它的熵也越大，这意味着它带来的信息量也越大（由于混乱程度很大，也就是说它表现为x的概率很低，反过来看就是当它为x时，已经是一个很有信息的事情了）

$$
H(x)\triangleq - \sum_{k=1}^Kp(X=k)log_2p(X=k)
$$

通常，当采用以2为底的log函数来计算熵时，每一个单元叫做bits；当采用e为底的log函数来计算熵时，每一个单元叫做nats。

例子：假设$X\in\{1 \dots 5\}$，其中它们的分布是p=[0.25, 0.25, 0.2, 0.15, 0.15]，那么这个变量的熵为：

$$
H(x) = P(x=1)log_2p(x=1) + p(x=2)log_2p(x=2) + p(x=3)log_2p(x=3) + p(x=4)log_2p(x=4) + p(x=5)log_2p(x=5)

=0.25 * log_{2}0.25 + 0.25 * log_{2}0.25 + 0.2 * log_{2}0.2 + 0.15 * log_{2}0.15 + 0.15 * log_{2}0.15

=2.2855
$$

###KL距离

KL距离用来描述两个分布之间的不相似性，也就是两个分布的距离。KL距离也被称为相对熵。

$$
KL(p||q) \triangleq \sum_{k=1}^Kp_klog\frac{p_k}{q_k}
$$

对上式子进行转换，得到：

$$
KL(p||q) = \sum_kp_klogp_k - \sum_kp_klogq_k = -H(p) + H(p,q)
$$

上式子中的H(p,q)又被称为交叉熵, 即：

$$
H(p,q) = -\sum_kp_klogq_k 
$$


###Mutual Information互信息
考虑两个随机变量，X和Y。假设我们想知道一个变量对另一个变量能表达多少，我们可以去计算相关系数，但是这只针对实数值的随机变量定义的，并且这个方法的局限性比较大。

一种比较通用的方法是计算p(X,Y)和p(X)p(Y)的相似度是多少，这也叫做互信息。

$$
I(X,Y) = KL(p(X,Y)||p(X)p(Y)) = \sum_x\sum_yp(x,y)log\frac{p(x,y)}{p(x)p(y)}
$$

当两个变量相互独立的时候，I(X,Y)=0（很明显，相互独立的时候，互相之间可以表达的信息就是0）

将上述式子进行简单变化可以得到：

$$
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
$$

其中，H(Y|X) 叫做条件熵，即：

$$
H(Y|X) = \sum_xp(x)H(Y|X=x)
$$
