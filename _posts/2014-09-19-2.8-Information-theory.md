---
layout: post
title: "2.8 Information theory"
category: mlapp
---

###Information theory

信息论与如何紧凑的数据表示有紧密关系，如数据压缩、源编码，同样与如何健壮的传输和存储也有着密切的关系。
看上去它与概率论、机器学习没有什么直接的关系，其实他们关系很密切的。
例如：在紧凑表示数据的时候，需要分配交短的编码给出现频率较高的字符，分配较长的编码给出现频率较低的字符。这跟自然语言处理中的处理方式很类似，a, an, the 这类词的编码要远小于不常见词的编码。
在Cover and Thomas 2006的书上有关于信息论的详细介绍，本节主要介绍熵、KL距离和互信息。

###熵

熵，用来表示混乱程度。当一个变量的混乱程度越大，则它的熵也越大，这意味着它带来的信息量也越大（由于混乱程度很大，也就是说它表现为x的概率很低，反过来看就是当它为x时，已经是一个很有信息的事情了）

$$
H(x)\triangleq - \sum_{k=1}^Kp(X=k)log_2p(X=k)
$$

通常，当采用以2为底的log函数来计算熵时，每一个单元叫做bits；当采用e为底的log函数来计算熵时，每一个单元叫做nats。

例子：假设$X\in\{1 \dots 5\}$，其中它们的分布是p=[0.25, 0.25, 0.2, 0.15, 0.15]，那么这个变量的熵为：
$$
H(x) = P(x=1)log_2p(x=1) + p(x=2)log_2p(x=2) + p(x=3)log_2p(x=3) + p(x=4)log_2p(x=4) + p(x=5)log_2p(x=5)
=0.25 * log_{2}0.25 + 0.25 * log_{2}0.25 + 0.2 * log_{2}0.2 + 0.15 * log_{2}0.15 + 0.15 * log_{2}0.15
=2.2855
$$

###KL距离

KL距离用来描述两个分布之间的不相似性，也就是两个分布的距离。KL距离也被称为相对熵。

$$
KL(p||q) \triangleq \sum_{k=1}^Kp_klog\frac{p_k}{q_k}
$$

对上式子进行转换，得到：

$$
KL(p||q) = \sum_kp_klogp_k - \sum_kp_klogq_k = -H(p) + H(p,q)
$$

上式子中的H(p,q)又被称为交叉熵.

