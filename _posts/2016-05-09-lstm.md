理解LSTM网络

###循环神经网络

人类并不是每次都是从头开始想事情。当你读到这篇文章时，你可以基于以前对这些词语的理解来读懂这篇文章，而不需要从头开始思考这些词的意思。这说明你的思路具有持久性。

传统的神经网络不能做这个事情，它看起来像是个主要缺点。例如，假定你需要对电影中每个点发生的事件进行分类。传统神经网络推理前一个事件的过程对于推理后面的事件是未知的。

循环神经网络解决了这个问题。它们是一种带有循环的网络，允许信息进行持久化。

图1

在上面的图中有一块神经网络A，x_t是输入，h_t是输出。A允许信息从网络中的一步传递给下一步。

这些循环使得循环神经网络有某种魔力。但是，如果你深入思考一下，事实证明他们并不是与普通神经网络不同。一个循环神经网络可以理解为同一个网络的多个拷贝，每一个网络传递一个信息给下一个。把循环拆开如下：

图2

这个链式特性说明循环神经网络与序列和列表是密切相关的。这样的原生结构就是适合这样的数据。

它们确实也被这样应用了。在过去几年中，有很多应用RNN成功解决一系列任务的例子：语音识别、语言模型、翻译、图像命名等等。更多精彩的讨论，可以看Andrej Karpathy的博客：[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

这些成功的关键在于使用了“LSTMs”，一种非常特殊类型的循环神经网络，比标准循环神经网络效果要好很多。几乎所有关于循环神经网络的激动人心的结果都使用了LSTM，这也是本文要讨论的内容。

###长词依赖带来的问题

应用RNN的一个初衷是它们可能将前面的信息引入到当前的任务中，例如利用视频前一帧的信息来推断当前帧的内容。如果RNN可以这样做，那就特么有用。但是，它们真的可以么？很难说。

有时候，我们只需要看最近的信息来完成当前的任务。例如，考虑一个语言模型试图基于前面的词来预测下一个词。如果我们试图预测“the clouds are in the SKY”中最后一个单词，我们不需要任何进一步的信息，很明显这个词应该是sky。在这样的场景中，相关信息与未知词的距离很小，RNN可以学习前面的信息。

图3

但是，同样有很多场景需要更多上下文。例如，我们试图推断“I grew up in France...I speak fluent French”中的最后一个词。最近的信息表示这个词可能是一个语言，但是当我们想限定哪一种语言时，我们就需要France这个信息。而这个所需的信息与我们要预测的内容可能相隔的特别远。

理论上，RNN绝对能够处理这样的“长词依赖”问题。人类可以通过细心的调参来解决这类问题。但是，实际中，RNN并不能学到这些。这个问题被[Hochreiter](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf)和[Bengio](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)深入研究了，他们还对“为什么这么难”做了基础理论分析。

庆幸的是，LSTM不存在这个问题。

###LSTM网络


















