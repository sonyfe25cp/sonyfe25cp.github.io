LSTM是一种特殊的循环神经网络，它可以学习长词依赖关系。最初由[Hochreiter & Schmidhuber]提出，经后续研究者的多次改进。现在已经在很多任务上效果表现很好，并被广泛应用。

LSTM被设计用于避免长词依赖的问题。记住长期的信息是它们自带的行为，并不是勉强为之。

所有的循环神经网络都有一个链式神经网络循环模块。在标准的RNN中，重复模块中只有一个简单的结构，例如只是单层tanh。

图

LSTMs同样有类似链式结构，但是重复模块部分结构不同。通常有更复杂的结合形式，这里有4个。

图

不要担心接下来的内容。我们将一步一步的学习LSTM。现在，开始熟悉相关概念。

图

在上面的图例中，每条线包含一个完整向量，从一个节点的输出到其他节点的输入。粉色圆圈代表点操作（pointwise），例如向量相加。黄色盒子表示神经网络学习层。汇合的线表示连接，而分开的线表示复制，另一份数据被复制到其他地方。

###LSTM后的核心

LSTM的关键在于细胞状态（cell state），也就是上图中顶部的水平线。

细胞状态是一种类似于传送带。它在整个链式结构中穿行，仅带有较小的线性操作。它较容易的让信息在其中不被改变的传输。

图

LSTM具有去除或者添加信息到细胞状态的能力，这由一个叫门（gate）的结构控制着。

门（gate）是一种让信息通过的可选装置。它由一个sigmoid神经网络层和一个点乘（pointwise multiplicatioon）操作构成。

图

Sigmoid层输出0到1之间的数，描述每个组件对信息放行的程度。0表示不放行，1表示放行全部。

一个LSTM有三个这样的门，保护和控制细胞状态。

###逐步深入LSTM

学习LSTM的第一步就是要决定什么样的信息需要从细胞状态中丢弃。这个决定由一个sigmoid层来完成，它叫做遗忘层（forget gate layer）。它处理$h_{t-1}$和$x_t$，为$C_{t-1}$输出一个0到1之间的数值。1表示完全保留，0表示完全抛弃。

回到语言模型中预测最后一个词的例子。在这类问题中，细胞状态应该包含前一个话题的类型，于是，正确的代词是可以利用。当遇到一个新的对象，就可以扔掉前一个对象了。

图

下一步是需要决定什么样的新信息需要在细胞状态中保留。这里包含两部分。首先，一个sigmoid层叫做输入门层（input gate layer）来决定什么样的数值将被更新。然后，一个tanh层为新侯选值创建个向量$\overline C_t$，可能用于加到状态上。下一步 ，我们将合并两者，用于更新状态。

在语言模型的例子中，我们希望将新对象的性质加到细胞状态中，替代已有的那个。

图

现在就需要更新就细胞状态$C_{t-1}$为$C_t$。

我们将旧状态乘以$f_t$，遗忘掉前面已经决定丢弃的信息。然后加上$i_t * \overline C_t$。这就是新的候选值，受我们决定每个状态值的程度影响。

图

最后，我们需要决定输出什么样的信息。输出结果会基于先前的细胞状态，但是将有一个过滤版本。首先，我们需要用一个sigmoid来决定哪部分的细胞状态可以被输出。然后，我们需要将细胞状态通过一个tanh，然后与另一个sigmod门的输出相乘，得到最终输出结果。

对于语言模型例子，由于它只是看到一个对象，它可能想要输出一个动词相关的信息，以防这就是所需要的。例如，它可能输出是否这个对象为单数或者复数，于是我们可以知道这个动词从形式应该用什么。

图

###LSTM上的变形

前面介绍的均为一个标准普通的LSTM。但是，并非所有的LSTM都和上面的一样。事实上，每篇论文中的LSTM都不一样。尽管差异比较小，但是值得去关注某些部分。

一个较出名的变形LSTM是[Gers & Schmidhuber]提出的，增加了'peephole connections'，这意味着我们让门层管理着细胞状态。

图

上面的途中，增加了peepholes给全部的门，但是很多论文中只给部分门，而不是全部。

另一种变形是增加双重遗忘和输出门。替代了分开决定什么需要被遗忘和应该添加什么信息，这里同时做这两个决定。只在需要输入什么的时候添加遗忘，只在遗忘某些旧信息的时候去添加新的值到状态中。

图

另一种更魔幻的变形是循环门单元或者叫GRU，由[Cho等人]提出。它合并了遗忘门和输入门到一个单独的更新门（update gate）。同样合并了细胞状态和隐含状态，还有一些其他变化。结果模型比标准LSTM更简单，也正在变得流行起来。

图

这只是其中较为出名的变形。还有很多其他的变形，如Depth Gated RNN。还有其他方法来跟踪长词依赖，例如Clockwork RNN。

哪一种变形更好呢？他们有什么区别么？Greff等人做了一个清晰的比较，发现他们都差不多。Jozefowicz等人测试了1w多种RNN结构，发现有些在任务上比LSTM效果要好。

###总结

前面，我提到了采用RNN带来的显著成果。重要的是他们都采用了LSTM。并且结果确实好。

写了一堆公式之后，LSTM看起来更吓人了。但通过一步步深入了解它，让它更容易理解。

LSTM是在采用RNN实现任务的一大步。让人好奇的是：另一大步在哪？研究者们的共识是：有的，另一大步就是attention。这个思路让RNN在每一步挑选信息时都会注意更大的信息集合。例如，当使用RNN来创建描述图片的标题是，它可能挑选图片的一部分，然后找词来描述它。事实上，Xu等人也这么做了，如果你想挖掘attention，这可能是个有趣的开始。已经有很多惊人的结果采用了attention，而且看上去还有更多。。。

Attention并不是RNN研究中唯一的亮点。例如，Grid LSTM看起来就特别赞。还有RNN的生成模型。过去几年中，RNN大放异彩，相信未来几年会更好。



















