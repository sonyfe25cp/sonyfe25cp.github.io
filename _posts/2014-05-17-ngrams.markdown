---
layout: post
title: "N-gram语言模型基本知识"
date: 2014-05-17 14:26:02
category: nlp
---

###背景

在使用ansj分词的时候，遇到了nlp分词不稳定的情况，而这个又确实的影响到了使用，为了解决这个问题，先把基础复习一下，也方便跟小伙伴讨论问题时候，思路是正确的。

###基础知识

一个语言模型通常构建为字符串s的概率分布p(s)，而这里的p(s)试图反映的时字符串s作为一个句子出现的概率。

    例子：在一个描述口语的语言模型中，如果一个人说的话中，每100个句子中就会有一个OK，那么我们可以认为p(OK)=0.01。而对于"An apple ate the chicken"，我们可以认为是概率为0，因为基本没人会这么说的。'''需要注意的是，语言模型跟语法没关系，即使语法对，也可以认为它出现的概率为0.'''

于是，对于一个由n个词构成的句子，它的概率公式可以这么表示：

$$
p(s) = p(w_1)p(w_2|w_1)p(w_2|w_1w_2)\dots p(w_n|pw_1\dots w_{n-1}) = \Pi_{i=1}^np(w_i|w_1\dots w_{n-1})
$$

一般的，前面的这n-1个词就叫做历史。而计算n-1个历史词是不现实的，因为样本里也不见得有这个句子。于是，通常的作法是简化这个条件概率。

如果n=2，那么词只与它前面的一个词有关系，这就是二元文法，也叫一阶马尔科夫链，记做bigram。

如果n=3，那么词与它前面两个词有关系，也就是三元文法，叫做二阶马尔科夫链，记做trigram。

###二元模型

二元用的最多，于是以二元为例，来记录一下语言模型的计算。由于i从1开始，但第一个单词前面没有词，于是添加一个BOS作为起始标识符，EOS作为结束标识符。

    例子：Mark wrote a book

$$
p(Mark wrote a book)=p(Mark|BOS)*p(wrote|Mark)*p(a|wrote)*p(book|a)*p(EOS|book)
$$

也就是公式：

$$
p(w_i|w_{i-1})=\frac{c(w_{i-1}w_i)}{\sum_{i}c(w_{i-1}w_i)}
$$

计算$p(w_i|w_{i-1})的时候，可以通过统计预料中的频次来得到。


###例子

语料：

    brown read holy bible

    mark read a book

    he read a book by david

计算概率：p(brown read a book)

$$
p(brown|BOS) = \frac{c(BOS brown)}{\sum_w c(BOS w)} = \frac{1}{3}
$$

$$
p(read|brown) = \frac{c(brown read)}{\sum_wc(brown w)} = \frac{1}{1}
$$

$$
p(a|read) = \frac{c(read a)}{\sum_wc(read w)} = \frac{2}{3}
$$

$$
p(book|a) = \frac{c(a book)}{\sum_wc(a w)} = \frac{1}{2}
$$

$$
p(EOS|book) = \frac{c(book EOS)}{\sum_wc(book w)} = \frac{1}{2}
$$

最终结果：

$$
p = 1/3 * 1 * 2/3 * 1/2 * 1/2  = 0.06
$$

###评价标准

首先将语言模型的概率连乘一下：
$$
p(s) = \Pi_{i=1}^{l+1}p(w_i|w_{i-n+1}^{i-1})
$$
其中，n是n-gram的n，l是词数。

然后，将由句子构成的测试集T中所有的句子都这么计算一次，得出测试集概率p(T):
$$
p(T)=\Pi_{i=1}^{l_T}p(t_i)
$$

交叉熵的测度可以通过下面公式：

$$
H_p(T) = -\frac{1}{W_T}log_2p(T)
$$


常用perplexity困惑度来度量语言模型。

模型的困惑度就是模型分配给测试集T中每一个词汇的概率的集合平均值的倒数，它和交叉熵的关系是：
$$
PP_T(T)= 2^{H_p(T)}
$$
